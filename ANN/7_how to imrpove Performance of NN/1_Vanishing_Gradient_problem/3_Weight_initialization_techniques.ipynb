{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier Glorat Weight Initialization #\n",
    "\n",
    "- Types : Normal & Uniform\n",
    "- Ensures that the scale of gradients remains consistent across all layers, helping to prevent issues with vanishing or exploding gradients.\n",
    "- initializes the weights using a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used of 'tanh' & 'uniform' activation functions\n",
    "- model.add(Dense(10,activation,kernel_initializer = 'glorat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# He Weight Initialization #\n",
    "\n",
    "- Types : Normal & Uniform\n",
    "- Particularly effective with ReLU activation functions, setting weights based on the number of input neurons to stabilize the training process.\n",
    "- initializes the weights using a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- used for 'ReLu' actication functions.\n",
    "- model.add(Dense(10,activation,kernel_initializer = 'he_normal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
